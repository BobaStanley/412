group_by(SubID) %>%
summarize(PropCorrCounting = mean(accuracy))
task_summary <- task_data %>%
filter(Include == 1) %>%
mutate(ComparisonCategory = case_when(
grepl("Before|After", Comparison) ~ "Order",
grepl("Smaller|Bigger", Comparison) ~ "Magnitude",
TRUE ~ NA_character_
)) %>%
mutate(SideCategory = ifelse(Side == "same", "Same", "Opposite")) %>%
mutate(TrialCategory = ifelse(grepl("consecutive", TrialType), "Consecutive", "Non-consecutive")) %>%
group_by(SubID, ComparisonCategory, SideCategory, TrialCategory) %>%
summarise(PropCorrYNTask = mean(Response.corr, na.rm = TRUE))
library(tidyverse)
counting <- read.csv("BABSYN_counting_tidy.csv")
task_data <- read.csv("BABSYN_YNtask_tidy.csv")
counting_summary <- counting %>%
filter(Include == 1) %>%  # Filter data for children with Include variable equal to 1
group_by(SubID) %>%
summarize(PropCorrCounting = mean(accuracy))
View(counting_summary)
task_summary <- task_data %>%
filter(Include == 1) %>%
mutate(ComparisonCategory = case_when(
grepl("Before|After", Comparison) ~ "Order",
grepl("Smaller|Bigger", Comparison) ~ "Magnitude",
TRUE ~ NA_character_
)) %>%
mutate(SideCategory = ifelse(Side == "same", "Same", "Opposite")) %>%
mutate(TrialCategory = ifelse(grepl("consecutive", TrialType), "Consecutive", "Non-consecutive")) %>%
group_by(SubID, ComparisonCategory, SideCategory, TrialCategory) %>%
summarise(PropCorrYNTask = mean(Response.corr, na.rm = TRUE))
combined_summary <- counting_summary %>%
inner_join(task_summary, by = "SubID")
View(combined_summary)
overall_scores <- task_data %>%
group_by(SubID) %>%
summarise(OverallScore = mean(PropCorrYNTask, na.rm = TRUE))
overall_scores <- task_summary %>%
group_by(SubID) %>%
summarise(OverallScore = mean(PropCorrYNTask, na.rm = TRUE))
# Create a scatter plot to visualize the relationship
ggplot(overall_scores, aes(x = Counting, y = OverallScore)) +
geom_point(aes(color = Counting), size = 3) +
geom_smooth(method = "lm", se = FALSE, color = "blue") +
labs(
title = "Correlation Between Counting Score and Order/Magnitude Judgment Score",
x = "Counting Score",
y = "Overall Order/Magnitude Judgment Score"
) +
scale_x_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.1)) +
scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.1)) +
theme_minimal() +
theme(
axis.title = element_text(size = 12),
axis.text = element_text(size = 10),
legend.position = "none"
)
View(task_summary)
library(tidyverse)
overall_scores <- task_summary %>%
group_by(SubID) %>%
summarise(OverallScore = mean(PropCorrYNTask, na.rm = TRUE))
# Create a scatter plot to visualize the relationship
ggplot(overall_scores, aes(x = ComparisonCategory, y = OverallScore)) +
geom_point(aes(color = Counting), size = 3) +
geom_smooth(method = "lm", se = FALSE, color = "blue") +
labs(
title = "Correlation Between Counting Score and Order/Magnitude Judgment Score",
x = "Counting Score",
y = "Overall Order/Magnitude Judgment Score"
) +
scale_x_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.1)) +
scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.1)) +
theme_minimal() +
theme(
axis.title = element_text(size = 12),
axis.text = element_text(size = 10),
legend.position = "none"
)
overall_scores <- task_summary %>%
group_by(SubID) %>%
summarise(OverallScore = mean(PropCorrYNTask, na.rm = TRUE))
# Create a scatter plot to visualize the relationship
ggplot(overall_scores, aes(x = ComparisonCategory, y = OverallScore)) +
geom_point(aes(color = ComparisonCategory), size = 3) +
geom_smooth(method = "lm", se = FALSE, color = "blue") +
labs(
title = "Correlation Between Counting Score and Order/Magnitude Judgment Score",
x = "Counting Score",
y = "Overall Order/Magnitude Judgment Score"
) +
scale_x_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.1)) +
scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.1)) +
theme_minimal() +
theme(
axis.title = element_text(size = 12),
axis.text = element_text(size = 10),
legend.position = "none"
)
# Calculate the overall score by taking the average proportion correct across all eight sub-tasks
counting_summary <- counting_summary %>%
filter(Include == 1) %>%
group_by(SubID) %>%
summarise(OverallCountingScore = mean(PropCorrCounting, na.rm = TRUE))
# Calculate the overall score by taking the average proportion correct across all eight sub-tasks
counting_summary <- counting_summary %>%
filter(include == 1) %>%
group_by(SubID) %>%
summarise(OverallCountingScore = mean(PropCorrCounting, na.rm = TRUE))
#this is just to give you an example of the code chunk header above, which is only necessary if you're submitting an assignment with code that won't run.
#for example, you might want to do that if you've made some progress on a problem, but aren't able to fully complete it. Rather than submitting nothing, submit what you've done so far.
library(tidyverse)
counting <- read.csv("BABSYN_counting_tidy.csv")
task_data <- read.csv("BABSYN_YNtask_tidy.csv")
counting_summary <- counting %>%
filter(Include == 1) %>%  # Filter data for children with Include variable equal to 1
group_by(SubID) %>%
summarize(PropCorrCounting = mean(accuracy))
task_summary <- task_data %>%
filter(Include == 1) %>%
mutate(ComparisonCategory = case_when(
grepl("Before|After", Comparison) ~ "Order",
grepl("Smaller|Bigger", Comparison) ~ "Magnitude",
TRUE ~ NA_character_
)) %>%
mutate(SideCategory = ifelse(Side == "same", "Same", "Opposite")) %>%
mutate(TrialCategory = ifelse(grepl("consecutive", TrialType), "Consecutive", "Non-consecutive")) %>%
group_by(SubID, ComparisonCategory, SideCategory, TrialCategory) %>%
summarise(PropCorrYNTask = mean(Response.corr, na.rm = TRUE))
glimpse(task_summary)
combined_summary <- counting_summary %>%
inner_join(task_summary, by = "SubID")
# PropCorrCounting repeats because it presents the proportion of correct responses in the counting task-- remains constant for that participant.
# Calculate the overall score by taking the average proportion correct across all eight sub-tasks
counting_summary <- counting_summary %>%
filter(Include == 1) %>%
group_by(SubID) %>%
summarise(OverallCountingScore = mean(PropCorrCounting, na.rm = TRUE))
library(tidyverse)
counting <- read.csv("BABSYN_counting_tidy.csv")
task_data <- read.csv("BABSYN_YNtask_tidy.csv")
counting_summary <- counting %>%
filter(Include == 1) %>%  # Filter data for children with Include variable equal to 1
group_by(SubID) %>%
summarize(PropCorrCounting = mean(accuracy))
task_summary <- task_data %>%
filter(Include == 1) %>%
mutate(ComparisonCategory = case_when(
grepl("Before|After", Comparison) ~ "Order",
grepl("Smaller|Bigger", Comparison) ~ "Magnitude",
TRUE ~ NA_character_
)) %>%
mutate(SideCategory = ifelse(Side == "same", "Same", "Opposite")) %>%
mutate(TrialCategory = ifelse(grepl("consecutive", TrialType), "Consecutive", "Non-consecutive")) %>%
group_by(SubID, ComparisonCategory, SideCategory, TrialCategory) %>%
summarise(PropCorrYNTask = mean(Response.corr, na.rm = TRUE))
glimpse(task_summary)
combined_summary <- counting_summary %>%
inner_join(task_summary, by = "SubID")
# Calculate the overall score by taking the average proportion correct across all eight sub-tasks
counting_summary <- counting_summary %>%
filter(Include == 1) %>%
group_by(SubID) %>%
summarise(OverallCountingScore = mean(PropCorrCounting, na.rm = TRUE))
View(counting)
View(counting_summary)
# Calculate the overall score by taking the average proportion correct across all eight sub-tasks
counting_summary <- counting %>%
filter(Include == 1) %>%
group_by(SubID) %>%
summarise(OverallCountingScore = mean(PropCorrCounting, na.rm = TRUE))
# Calculate the overall score by taking the average proportion correct across all eight sub-tasks
counting_summary <- counting %>%
filter(Include == 1) %>%
group_by(SubID) %>%
summarise(OverallCountingScore = mean(PropCorrCounting, na.rm = TRUE))
View(counting_summary)
# Calculate the overall score by taking the average proportion correct across all eight sub-tasks
counting_summary <- counting %>%
filter(Include == 1) %>%
group_by(SubID) %>%
summarise(OverallCountingScore = mean(Counting, na.rm = TRUE))
# Calculate the overall score from the "task_summary" dataset
task_summary <- task_data %>%
filter(Include == 1) %>%
group_by(SubID) %>%
summarise(OverallTaskScore = mean(PropCorrYNTask, na.rm = TRUE))
View(task_data)
task_summary <- task_data %>%
filter(Include == 1) %>%
mutate(ComparisonCategory = case_when(
grepl("Before|After", Comparison) ~ "Order",
grepl("Smaller|Bigger", Comparison) ~ "Magnitude",
TRUE ~ NA_character_
)) %>%
mutate(SideCategory = ifelse(Side == "same", "Same", "Opposite")) %>%
mutate(TrialCategory = ifelse(grepl("consecutive", TrialType), "Consecutive", "Non-consecutive")) %>%
group_by(SubID, ComparisonCategory, SideCategory, TrialCategory) %>%
summarise(PropCorrYNTask = mean(Response.corr, na.rm = TRUE))
counting_summary <- counting %>%
filter(Include == 1) %>%  # Filter data for children with Include variable equal to 1
group_by(SubID) %>%
summarize(PropCorrCounting = mean(accuracy))
# Calculate the overall counting score
counting_summary <- counting %>%
filter(Include == 1) %>%
group_by(SubID) %>%
summarize(PropCorrCounting = mean(accuracy))
# Calculate the overall task score
task_summary <- task_data %>%
filter(Include == 1) %>%
mutate(ComparisonCategory = case_when(
grepl("Before|After", Comparison) ~ "Order",
grepl("Smaller|Bigger", Comparison) ~ "Magnitude",
TRUE ~ NA_character_
)) %>%
mutate(SideCategory = ifelse(Side == "same", "Same", "Opposite")) %>%
mutate(TrialCategory = ifelse(grepl("consecutive", TrialType), "Consecutive", "Non-consecutive")) %>%
group_by(SubID, ComparisonCategory, SideCategory, TrialCategory) %>%
summarise(PropCorrYNTask = mean(Response.corr, na.rm = TRUE))
# Merge the datasets based on 'SubID'
merged_data <- counting_summary %>%
inner_join(task_summary, by = "SubID")
# Create a scatter plot to explore the hypothesis
ggplot(merged_data, aes(x = PropCorrCounting, y = PropCorrYNTask)) +
geom_point(color = "blue", size = 3) +  # Scatter plot points
geom_smooth(method = "lm", color = "red", se = FALSE) +  # Linear regression line
labs(
title = "Correlation Between Counting Score and Task Score",
x = "Overall Counting Score",
y = "Overall Task Score"
) +
scale_x_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.1)) +
scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.1)) +
theme_minimal() +
theme(
axis.title = element_text(size = 12),
axis.text = element_text(size = 10),
legend.position = "none"
)
# Calculate the overall counting score
counting_summary <- counting %>%
filter(Include == 1) %>%
group_by(SubID) %>%
summarize(PropCorrCounting = mean(accuracy))
# Calculate the overall task score
task_summary <- task_data %>%
filter(Include == 1) %>%
mutate(ComparisonCategory = case_when(
grepl("Before|After", Comparison) ~ "Order",
grepl("Smaller|Bigger", Comparison) ~ "Magnitude",
TRUE ~ NA_character_
)) %>%
mutate(SideCategory = ifelse(Side == "same", "Same", "Opposite")) %>%
mutate(TrialCategory = ifelse(grepl("consecutive", TrialType), "Consecutive", "Non-consecutive")) %>%
group_by(SubID, ComparisonCategory, SideCategory, TrialCategory) %>%
summarise(PropCorrYNTask = mean(Response.corr, na.rm = TRUE))
# Merge the datasets based on 'SubID'
merged_data <- counting_summary %>%
inner_join(task_summary, by = "SubID")
# Create a scatter plot to explore the hypothesis
ggplot(merged_data, aes(x = PropCorrCounting, y = PropCorrYNTask)) +
geom_point(color = "blue", size = 3) +  # Scatter plot points
geom_smooth(method = "lm", color = "red", se = FALSE) +  # Linear regression line
labs(
title = "Correlation Between Counting Score and Task Score",
x = "Overall Counting Score",
y = "Overall Task Score"
) +
scale_x_continuous(limits = c(0, 2), breaks = seq(0, 1, 0.1)) +
scale_y_continuous(limits = c(0, 2), breaks = seq(0, 1, 0.1)) +
theme_minimal() +
theme(
axis.title = element_text(size = 12),
axis.text = element_text(size = 10),
legend.position = "none"
)
# Calculate the overall counting score
counting_summary <- counting %>%
filter(Include == 1) %>%
group_by(SubID) %>%
summarize(PropCorrCounting = mean(accuracy))
# Calculate the overall task score
task_summary <- task_data %>%
filter(Include == 1) %>%
mutate(ComparisonCategory = case_when(
grepl("Before|After", Comparison) ~ "Order",
grepl("Smaller|Bigger", Comparison) ~ "Magnitude",
TRUE ~ NA_character_
)) %>%
mutate(SideCategory = ifelse(Side == "same", "Same", "Opposite")) %>%
mutate(TrialCategory = ifelse(grepl("consecutive", TrialType), "Consecutive", "Non-consecutive")) %>%
group_by(SubID, ComparisonCategory, SideCategory, TrialCategory) %>%
summarise(PropCorrYNTask = mean(Response.corr, na.rm = TRUE))
# Merge the datasets based on 'SubID'
merged_data <- counting_summary %>%
inner_join(task_summary, by = "SubID")
# Create a scatter plot to explore the hypothesis
ggplot(merged_data, aes(x = PropCorrCounting, y = PropCorrYNTask)) +
geom_point(color = "blue", size = 3) +  # Scatter plot points
geom_smooth(method = "lm", color = "red", se = FALSE) +  # Linear regression line
labs(
title = "Correlation Between Counting Score and Task Score",
x = "Overall Counting Score",
y = "Overall Task Score"
) +
scale_x_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.1)) +
scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.1)) +
theme_minimal() +
theme(
axis.title = element_text(size = 12),
axis.text = element_text(size = 10),
legend.position = "none"
)
# Create a scatter plot to explore the hypothesis
ggplot(merged_data, aes(x = PropCorrCounting, y = PropCorrYNTask, color = ComparisonCategory)) +
geom_point(size = 3) +
geom_smooth(method = "lm", se = FALSE) +
labs(
title = "Correlation Between Counting Score and Task Score",
x = "Overall Counting Score",
y = "Overall Task Score"
) +
scale_x_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.1)) +
scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.1)) +
theme_minimal() +
theme(
axis.title = element_text(size = 12),
axis.text = element_text(size = 10)
)
# Create separate faceted plots for Order and Magnitude tasks
ggplot(merged_data, aes(x = PropCorrCounting, y = PropCorrYNTask, color = ComparisonCategory)) +
geom_point(size = 3) +
geom_smooth(method = "lm", se = FALSE) +
labs(
title = "Correlation Between Counting Score and Task Score",
x = "Overall Counting Score",
y = "Overall Task Score"
) +
scale_x_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.1)) +
scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.1)) +
theme_minimal() +
theme(
axis.title = element_text(size = 12),
axis.text = element_text(size = 10)
) +
facet_wrap(~ComparisonCategory)
sameside_data <- task_summary %>%
filter(SideCategory == "Same")
# Create a data visualization to communicate the pattern
ggplot(same_side_data, aes(x = TrialCategory, y = PropCorrYNTask)) +
geom_boxplot(fill = "lightblue", color = "blue", width = 0.6, outlier.shape = NA) +
geom_jitter(position = position_jitter(width = 0.2), color = "blue", size = 3) +
stat_summary(fun = mean, geom = "point", color = "red", size = 3) +
labs(
title = "Task Performance Pattern for Same Side Trials",
x = "Trial Type",
y = "Proportion Correct (Mean)"
) +
scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.1)) +
theme_minimal() +
theme(
axis.title = element_text(size = 12),
axis.text = element_text(size = 10)
#this is just to give you an example of the code chunk header above, which is only necessary if you're submitting an assignment with code that won't run.
#for example, you might want to do that if you've made some progress on a problem, but aren't able to fully complete it. Rather than submitting nothing, submit what you've done so far.
library(tidyverse)
counting <- read.csv("BABSYN_counting_tidy.csv")
task_data <- read.csv("BABSYN_YNtask_tidy.csv")
counting_summary <- counting %>%
filter(Include == 1) %>%  # Filter data for children with Include variable equal to 1
group_by(SubID) %>%
summarize(PropCorrCounting = mean(accuracy))
task_summary <- task_data %>%
filter(Include == 1) %>%
mutate(ComparisonCategory = case_when(
grepl("Before|After", Comparison) ~ "Order",
grepl("Smaller|Bigger", Comparison) ~ "Magnitude",
TRUE ~ NA_character_
)) %>%
mutate(SideCategory = ifelse(Side == "same", "Same", "Opposite")) %>%
mutate(TrialCategory = ifelse(grepl("consecutive", TrialType), "Consecutive", "Non-consecutive")) %>%
group_by(SubID, ComparisonCategory, SideCategory, TrialCategory) %>%
summarise(PropCorrYNTask = mean(Response.corr, na.rm = TRUE))
glimpse(task_summary)
combined_summary <- counting_summary %>%
inner_join(task_summary, by = "SubID")
# PropCorrCounting repeats because it presents the proportion of correct responses in the counting task-- remains constant for that participant.
# Calculate the overall counting score
counting_summary <- counting %>%
filter(Include == 1) %>%
group_by(SubID) %>%
summarize(PropCorrCounting = mean(accuracy))
# Calculate the overall task score
task_summary <- task_data %>%
filter(Include == 1) %>%
mutate(ComparisonCategory = case_when(
grepl("Before|After", Comparison) ~ "Order",
grepl("Smaller|Bigger", Comparison) ~ "Magnitude",
TRUE ~ NA_character_
)) %>%
mutate(SideCategory = ifelse(Side == "same", "Same", "Opposite")) %>%
mutate(TrialCategory = ifelse(grepl("consecutive", TrialType), "Consecutive", "Non-consecutive")) %>%
group_by(SubID, ComparisonCategory, SideCategory, TrialCategory) %>%
summarise(PropCorrYNTask = mean(Response.corr, na.rm = TRUE))
# Merge the datasets based on 'SubID'
merged_data <- counting_summary %>%
inner_join(task_summary, by = "SubID")
# Create a scatter plot to explore the hypothesis
ggplot(merged_data, aes(x = PropCorrCounting, y = PropCorrYNTask)) +
geom_point(color = "blue", size = 3) +  # Scatter plot points
geom_smooth(method = "lm", color = "red", se = FALSE) +  # Linear regression line
labs(
title = "Correlation Between Counting Score and Task Score",
x = "Overall Counting Score",
y = "Overall Task Score"
) +
scale_x_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.1)) +
scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.1)) +
theme_minimal() +
theme(
axis.title = element_text(size = 12),
axis.text = element_text(size = 10),
legend.position = "none"
)
# Create a scatter plot to explore the hypothesis
ggplot(merged_data, aes(x = PropCorrCounting, y = PropCorrYNTask, color = ComparisonCategory)) +
geom_point(size = 3) +
geom_smooth(method = "lm", se = FALSE) +
labs(
title = "Correlation Between Counting Score and Task Score",
x = "Overall Counting Score",
y = "Overall Task Score"
) +
scale_x_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.1)) +
scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.1)) +
theme_minimal() +
theme(
axis.title = element_text(size = 12),
axis.text = element_text(size = 10)
)
# Create separate faceted plots for Order and Magnitude tasks
ggplot(merged_data, aes(x = PropCorrCounting, y = PropCorrYNTask, color = ComparisonCategory)) +
geom_point(size = 3) +
geom_smooth(method = "lm", se = FALSE) +
labs(
title = "Correlation Between Counting Score and Task Score",
x = "Overall Counting Score",
y = "Overall Task Score"
) +
scale_x_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.1)) +
scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.1)) +
theme_minimal() +
theme(
axis.title = element_text(size = 12),
axis.text = element_text(size = 10)
) +
facet_wrap(~ComparisonCategory)
sameside_data <- task_summary %>%
filter(SideCategory == "Same")
# Create a data visualization to communicate the pattern
ggplot(same_side_data, aes(x = TrialCategory, y = PropCorrYNTask)) +
geom_boxplot(fill = "lightblue", color = "blue", width = 0.6, outlier.shape = NA) +
geom_jitter(position = position_jitter(width = 0.2), color = "blue", size = 3) +
stat_summary(fun = mean, geom = "point", color = "red", size = 3) +
labs(
title = "Task Performance Pattern for Same Side Trials",
x = "Trial Type",
y = "Proportion Correct (Mean)"
) +
scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.1)) +
theme_minimal() +
theme(
axis.title = element_text(size = 12),
axis.text = element_text(size = 10)
sameside_data <- task_summary %>%
filter(SideCategory == "Same")
# Create a data visualization to communicate the pattern
ggplot(same_side_data, aes(x = TrialCategory, y = PropCorrYNTask)) +
geom_boxplot(fill = "lightblue", color = "blue", width = 0.6, outlier.shape = NA) +
geom_jitter(position = position_jitter(width = 0.2), color = "blue", size = 3) +
stat_summary(fun = mean, geom = "point", color = "red", size = 3) +
labs(
title = "Task Performance Pattern for Same Side Trials",
x = "Trial Type",
y = "Proportion Correct (Mean)"
) +
scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.1)) +
theme_minimal() +
theme(
axis.title = element_text(size = 12),
axis.text = element_text(size = 10)
)
sameside_data <- task_summary %>%
filter(SideCategory == "Same")
# Create a data visualization to communicate the pattern
ggplot(sameside_data, aes(x = TrialCategory, y = PropCorrYNTask)) +
geom_boxplot(fill = "lightblue", color = "blue", width = 0.6, outlier.shape = NA) +
geom_jitter(position = position_jitter(width = 0.2), color = "blue", size = 3) +
stat_summary(fun = mean, geom = "point", color = "red", size = 3) +
labs(
title = "Task Performance Pattern for Same Side Trials",
x = "Trial Type",
y = "Proportion Correct (Mean)"
) +
scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.1)) +
theme_minimal() +
theme(
axis.title = element_text(size = 12),
axis.text = element_text(size = 10)
)
